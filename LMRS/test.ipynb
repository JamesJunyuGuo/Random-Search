{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-30 16:57:33,847] Making new env: HalfCheetah-v1\n",
      "/opt/anaconda3/envs/lmrs/lib/python3.12/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "2024-09-30 16:57:33,882\tINFO worker.py:1619 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_name': 'HalfCheetah-v1', 'seed': 4242, 'policy_type': 'linear', 'dir_std': 0.03, 'step_size': 0.02, 'num_workers': 8, 'rollout_length': 1000, 'num_rollouts': 8, 'sampler': 'jacobian', 'n_iter': 2500, 'every_val': 10, 'shift': 0, 'optimizer': 'sgd', 'learning_rate': 0.01, 'num_hidden_dim': 8, 'num_learning_iterations': 10, 'gram_schmidt': True, 'variance_reduced': False, 'filter_corrected': True}\n",
      "Sampler is jacobian; so a learner is created\n",
      "Learner with 102->8->1 using SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: True\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ") with gs:True\n",
      "TOP-K is 8\n"
     ]
    }
   ],
   "source": [
    "from random_search.mujoco_random_search_learned import MujocoRandomSearchLearned\n",
    "import click\n",
    "import json\n",
    "import gym\n",
    "import ray \n",
    "\n",
    "# @click.command()\n",
    "# @click.option('--param_file', default='params.json', help='JSON file for exp parameters')\n",
    "\n",
    "param_file = 'hc.json'  # Replace with the actual path to your JSON file\n",
    "\n",
    "# Correct usage of the with statement\n",
    "with open(param_file, 'r') as json_params:\n",
    "    params = json.load(json_params)\n",
    "    print(params)\n",
    "    \n",
    "\n",
    "exp_identifier = '|'.join('{}={}'.format(key,val) for (key,val) in params.items())\n",
    "params['exp_id'] = exp_identifier\n",
    "\n",
    "env = gym.make(params['env_name'])\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "# set policy parameters. Possible filters: 'MeanStdFilter' for v2, 'NoFilter' for v1.\n",
    "policy_params={'type':'linear',\n",
    "                'ob_filter': \"MeanStdFilter\",\n",
    "                'ob_dim':obs_dim,\n",
    "                'ac_dim':act_dim}\n",
    "params[\"policy_params\"] = policy_params\n",
    "params[\"dimension\"] = obs_dim*act_dim\n",
    "# ray.init(num_cpus=params[\"num_workers\"], ignore_reinit_error=True, dashboard_host=\"127.0.0.1\")\n",
    "\n",
    "\n",
    "model = MujocoRandomSearchLearned(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.get_weights_plus_stats().shape\n",
    "policy = model.policy\n",
    "policy.get_weights_plus_stats().shape\n",
    "policy.observation_filter.get_stats()\n",
    "\n",
    "mu,std = policy.observation_filter.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 17)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mu, std = policy.observation_filter.get_stats()\n",
    "mu_expanded = mu[np.newaxis, :]  # Shape becomes (1, 17)\n",
    "std_expanded = std[np.newaxis, :]  # Shape becomes (1, 17)\n",
    "\n",
    "# # Concatenate weights, mu, and std along the first axis (rows)\n",
    "aux = np.concatenate([policy.weights, mu_expanded, std_expanded], axis=0)\n",
    "aux.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AverageRewards': -0.7374557836879756, 'StdRewards': 0.7436504429711981, 'MaxRewards': 1.378270869378289, 'MinRewards': -2.2905339439886228}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_episodes = 0\n",
    "model.sampler.current_solution = torch.from_numpy(model.current_solution).view(1,-1).float()\n",
    "iteration = 0\n",
    "model.ts = iteration + 1\n",
    "validation_epoch = 10\n",
    "if iteration % validation_epoch == 0:\n",
    "    # Evaluate at every validation_epoch\n",
    "    #print('Evaluation at {}'.format(num_episodes))\n",
    "    model.evaluate(num_episodes) \n",
    "\n",
    "directions = model.sampler.sample()\n",
    "directions.shape\n",
    "# 8*6*17\n",
    "rewards, num_eval = model.rollouts(directions)\n",
    "num_episodes+=num_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2326.3784)\n",
      "tensor(2326.3784)\n"
     ]
    }
   ],
   "source": [
    "model.update(rewards, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.post_iteration_cleanup()\n",
    "model.train_stats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to evaluate the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "policy_id = ray.put(model.current_solution)\n",
    "rollout_per_worker = int(50/model.num_workers) + 1\n",
    "\n",
    "rollouts = [ worker.do_rollouts_same_policy.remote(policy_id, None,\n",
    "                                        num_rollouts=rollout_per_worker, \n",
    "                                        evaluate=True) for worker in model.workers]\n",
    "\n",
    "results = ray.get(rollouts)\n",
    "\n",
    "rewards = []\n",
    "for result in results:\n",
    "    rewards += result[\"rollout_rewards\"]\n",
    "\n",
    "rewards = np.array(rewards, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the sampler work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = model.sampler\n",
    "sampler.num_directions\n",
    "sampler.dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to rollout      \n",
    "select different directions $d_i$ and cast perturbations on the original parameters. Collect rollouts as an estimation for the value function $R_{\\pi_{\\theta}}(\\tau)$. And finally we can estimate the derivative with respect to $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions_n = directions.numpy()\n",
    "rollout_per_worker = int(directions_n.shape[0]/model.num_workers)\n",
    "# Current implementation is incomplete and only support this\n",
    "assert(rollout_per_worker*model.num_workers == directions_n.shape[0])\n",
    "\n",
    "# Sync all workers first\n",
    "current_policy = ray.put(model.current_solution)\n",
    "\n",
    "# Do rollouts\n",
    "rollouts = []\n",
    "for rollout in range(rollout_per_worker):\n",
    "    for worker in range(model.num_workers):\n",
    "        perturbation_for_worker = ray.put(directions_n[worker+rollout*model.num_workers])\n",
    "        rollouts+= [model.workers[worker].do_rollouts_same_policy.remote(current_policy, perturbation_for_worker, evaluate=False)]\n",
    "\n",
    "results = ray.get(rollouts)\n",
    "\n",
    "pos_rollouts_un = []\n",
    "neg_rollouts_un = []\n",
    "\n",
    "pos_rollouts = []\n",
    "neg_rollouts = []\n",
    "time = 0\n",
    "for result in results:\n",
    "    time += result['steps']\n",
    "    pos_rollouts += [result['rollout_rewards'][0]['+']]\n",
    "    neg_rollouts += [result['rollout_rewards'][0]['-']]\n",
    "\n",
    "    pos_rollouts_un += [result['rollout_rewards'][0]['un_+']]\n",
    "    neg_rollouts_un += [result['rollout_rewards'][0]['un_-']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update a policy\n",
    "How to use the obtained reward data and the directions to update the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7072)\n",
      "tensor(0.7072)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 102])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.update(rewards, directions)\n",
    "mx_rewards = torch.max(rewards['+'], rewards['-'])\n",
    "ss, ind = torch.sort(mx_rewards, dim=0, descending=True)\n",
    "chosen_indices = ind[0:model.top_k,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7072)\n",
      "tensor(0.7072)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directions = directions[chosen_indices,:]\n",
    "rewards['+'] = rewards['+'][chosen_indices,:]\n",
    "rewards['-'] = rewards['-'][chosen_indices,:]\n",
    "rewards['un_+'] = rewards['un_+'][chosen_indices,:]\n",
    "rewards['un_-'] = rewards['un_-'][chosen_indices,:]\n",
    "\n",
    "stddev = torch.std(torch.cat((rewards['+'], rewards['-']),0), unbiased=False)\n",
    "print(stddev)\n",
    "if stddev < 1:\n",
    "    stddev = 1\n",
    "rewards['+'] /= stddev\n",
    "rewards['-'] /= stddev\n",
    "directional_grads = rewards['+'] - rewards['-']\n",
    "\n",
    "final_direction = torch.matmul(directions.transpose(0,1), directional_grads)\n",
    "final_direction = final_direction / directions.shape[0]\n",
    "final_direction = final_direction / model.dir_std \n",
    "\n",
    "if len(model.old_gradients) > 5:\n",
    "    model.old_gradients.pop(0)\n",
    "model.old_gradients.append(final_direction)\n",
    "\n",
    "update = model.step_size * final_direction\n",
    "model.current_solution += update.numpy().reshape(model.current_solution.shape)\n",
    "\n",
    "if not \"unit_normal\" in model.sampler_type:\n",
    "    model.update_models(model.current_solution, directions, rewards)\n",
    "\n",
    "stddev2 = torch.std(torch.cat((rewards['un_+'], rewards['un_-']),0), unbiased=False)\n",
    "print(stddev2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mlearner\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.learner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
